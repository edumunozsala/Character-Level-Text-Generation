{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-level text generator with Pytorch\n",
    "\n",
    "# A guide on Recurrent Neural Network\n",
    "\n",
    "In this notebook we will be implementing a simple RNN character model with PyTorch to familiarize ourselves with the PyTorch library and get started with RNNs. The goal is to build a model that can complete your sentence based on a few characters or a word used as input.\n",
    "\n",
    "The model will be fed with a word and will predict what the next character in the sentence will be. This process will repeat itself until we generate a sentence of our desired length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import random as rnd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import string\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset\n",
    "\n",
    "First, we'll define the sentences that we want our model to output when fed with the first word or the first few characters. Our dataset is a text file containing Shakespeare's plays or books that we will extract sequence of chars to use as input to our model. Then our model will learn how to complete sentences like \"Shakespeare would do\". \n",
    "\n",
    "This dataset can be downloaded from Karpathy's Github account: https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt.\n",
    "\n",
    "As in many of my notebooks, we set some variables to the data directory and filenames. If you want to run this code on your own enviroment you must change these values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the root folder\n",
    "root_folder='.'\n",
    "# Set the folder with the dataset\n",
    "data_folder_name='data'\n",
    "model_folder_name='model'\n",
    "# Set the filename\n",
    "filename='input.txt'\n",
    "\n",
    "# Path to the data folder\n",
    "DATA_PATH = os.path.abspath(os.path.join(root_folder, data_folder_name))\n",
    "model_dir = os.path.abspath(os.path.join(root_folder, model_folder_name))\n",
    "\n",
    "# Set the path where the text for training is stored\n",
    "train_path = os.path.join(DATA_PATH, filename)\n",
    "\n",
    "# Set a seed\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_data(filename, init_dialog=False):\n",
    "    ''' Load the texts from the filename, splitting by lines and removing empty strings.\n",
    "        Setting init_dialog = True will remove lines where the character who is going to speak is indicated\n",
    "    '''\n",
    "    sentences = []\n",
    "    with open(filename, 'r') as reader:\n",
    "        #sentences = reader.readlines()\n",
    "        for line in reader:\n",
    "            #if ':' not in line and line !='\\n':\n",
    "            if init_dialog or ':' not in line:\n",
    "                # Append the line to the sentences, removing the end of line character\n",
    "                sentences.append(line[:-1])\n",
    "                \n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the input data, sentences from Shakespeare's plays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:  29723\n",
      "['Before we proceed any further, hear me speak.', '', 'Speak, speak.', '', 'You are all resolved rather to die than to famish?', '', 'Resolved. resolved.', '', 'First, you know Caius Marcius is chief enemy to the people.', '', \"We know't, we know't.\", '', \"Let us kill him, and we'll have corn at our own price.\", \"Is't a verdict?\", '', '', 'One word, good citizens.', '', 'We are accounted poor citizens, the patricians good.', 'would yield us but the superfluity, while it were']\n"
     ]
    }
   ],
   "source": [
    "sentences = load_text_data(train_path)\n",
    "print('Number of sentences: ', len(sentences))\n",
    "print(sentences[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the input data\n",
    "\n",
    "When working with text data, we usually need to perform some cleanings to prepare the data for our algorithm.This time we will start with a simple cleaning, convert to lowercase the text and remove non alphanumeric chracters (a parameter configuration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sentences, alpha=False):\n",
    "    ''' Cleaning process of the text'''\n",
    "    if alpha:\n",
    "        # Remove non alphabetic character\n",
    "        cleaned_text = [''.join([t.lower() for t in text if t.isalpha() or t.isspace()]) for text in sentences]\n",
    "    else:\n",
    "        # Simply lower the characters\n",
    "        cleaned_text = [t.lower() for t in sentences]\n",
    "    # Remove any emoty string\n",
    "    cleaned_text = [t for t in cleaned_text if t!='']\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters:  894876\n",
      "before we proceed any further, hear me speak. speak, speak. you are all resolved rather to die than \n"
     ]
    }
   ],
   "source": [
    "# Clean the sentences\n",
    "sentences = clean_text(sentences, False)\n",
    "# Join all the sentences in a one long string\n",
    "sentences = ' '.join(sentences)\n",
    "print('Number of characters: ', len(sentences))\n",
    "print(sentences[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dictionary\n",
    "\n",
    "Now we'll create a dictionary out of all the characters that we have in the sentences and map them to an integer. This will allow us to convert our input characters to their respective integers (char2int) and viceversa (int2char)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharVocab: \n",
    "    ''' Create a Vocabulary for '''\n",
    "    def __init__(self, type_vocab,pad_token='<PAD>', eos_token='<EOS>', unk_token='<UNK>'): #Initialization of the type of vocabulary\n",
    "        self.type = type_vocab\n",
    "        #self.int2char ={}\n",
    "        self.int2char = []\n",
    "        if pad_token !=None:\n",
    "            self.int2char += [pad_token]\n",
    "        if eos_token !=None:\n",
    "            self.int2char += [eos_token]\n",
    "        if unk_token !=None:\n",
    "            self.int2char += [unk_token]\n",
    "        self.char2int = {}\n",
    "        \n",
    "    def __call__(self, text):       #When called, adds the values of parameters x_1 and x_2, prints and returns the result \n",
    "        # Join all the sentences together and extract the unique characters from the combined sentences\n",
    "        chars = set(''.join(text))\n",
    "\n",
    "        # Creating a dictionary that maps integers to the characters\n",
    "        self.int2char += list(chars)\n",
    "\n",
    "        # Creating another dictionary that maps characters to integers\n",
    "        self.char2int = {char: ind for ind, char in enumerate(self.int2char)}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary:  38\n",
      "Int to Char:  ['<UNK>', 'l', 'g', 'b', 'z', '-', 'k', 'e', 'o', 'r', 'i', ',', 'w', \"'\", ';', 'f', 'x', '?', 'd', '$', '3', 'a', 's', 'v', 'm', '&', 't', 'h', 'u', 'p', 'n', 'y', ' ', 'c', 'q', '!', 'j', '.']\n",
      "Char to Int:  {'<UNK>': 0, 'l': 1, 'g': 2, 'b': 3, 'z': 4, '-': 5, 'k': 6, 'e': 7, 'o': 8, 'r': 9, 'i': 10, ',': 11, 'w': 12, \"'\": 13, ';': 14, 'f': 15, 'x': 16, '?': 17, 'd': 18, '$': 19, '3': 20, 'a': 21, 's': 22, 'v': 23, 'm': 24, '&': 25, 't': 26, 'h': 27, 'u': 28, 'p': 29, 'n': 30, 'y': 31, ' ': 32, 'c': 33, 'q': 34, '!': 35, 'j': 36, '.': 37}\n"
     ]
    }
   ],
   "source": [
    "vocab = CharVocab('char',None,None,'<UNK>')\n",
    "vocab(sentences)\n",
    "print('Length of vocabulary: ', len(vocab.int2char))\n",
    "print('Int to Char: ', vocab.int2char)\n",
    "print('Char to Int: ', vocab.char2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the dictionary\n",
    "\n",
    "In this example it is nor mandatory to save the dictionary inmediately, because it is a quick task. But when dealing with a huge corpus and a large dictionary, we should save the dictionary to restore it latter when new experiments will be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check or create the directory where dictionary will be saved\n",
    "if not os.path.exists(DATA_PATH): # Make sure that the folder exists\n",
    "    os.makedirs(DATA_PATH)\n",
    "    \n",
    "# Save the dictionary to data path dir  \n",
    "with open(os.path.join(DATA_PATH, 'char_dict.pkl'), \"wb\") as f:\n",
    "    pickle.dump(vocab.char2int, f)\n",
    "\n",
    "with open(os.path.join(DATA_PATH, 'int_dict.pkl'), \"wb\") as f:\n",
    "    pickle.dump(vocab.int2char, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the input data and labels for training\n",
    "\n",
    "As we're going to predict the next character in the sequence at each time step, we'll have to divide each sentence into\n",
    "\n",
    "- **Input data**: The last input character should be excluded as it does not need to be fed into the model\n",
    "- **Target/Ground Truth Label**: One time-step ahead of the Input data as this will be the \"correct answer\" for the model at each time step corresponding to the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(indices, dict_size):\n",
    "    ''' Define one hot encode matrix for our sequences'''\n",
    "    # Creating a multi-dimensional array with the desired output shape\n",
    "    # Encode every integer with its one hot representation\n",
    "    features = np.eye(dict_size, dtype=np.float32)[indices.flatten()]\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    features = features.reshape((*indices.shape, dict_size))\n",
    "            \n",
    "    return features\n",
    "\n",
    "def encode_text(input_text, vocab, one_hot = False):\n",
    "    ''' Encode the input_text replacing the char by its integer number based on the dictionary vocab'''\n",
    "    # Replace every char by its integer value based on the vocabulary\n",
    "    output = [vocab.char2int.get(character,0) for character in input_text]\n",
    "    \n",
    "    if one_hot:\n",
    "    # One hot encode every integer of the sequence\n",
    "        dict_size = len(vocab.char2int)\n",
    "        return one_hot_encode(output, dict_size)\n",
    "    else:\n",
    "        return np.array(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can encode our text, replacing every character by the integer value in the dictionary. When we have our dataset unified and prepared, we should do a quick check and see an example of the data our model will be trained on. This is generally a good idea as it allows you to see how each of the further processing steps affects the reviews and it also ensures that the data has been loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original text:\n",
      "before we proceed any further, hear me speak. speak, speak. you are all resolved rather to die than \n",
      "\n",
      "Encoded text:\n",
      "[ 3  7 15  8  9  7 32 12  7 32 29  9  8 33  7  7 18 32 21 30 31 32 15 28\n",
      "  9 26 27  7  9 11 32 27  7 21  9 32 24  7 32 22 29  7 21  6 37 32 22 29\n",
      "  7 21  6 11 32 22 29  7 21  6 37 32 31  8 28 32 21  9  7 32 21  1  1 32\n",
      "  9  7 22  8  1 23  7 18 32  9 21 26 27  7  9 32 26  8 32 18 10  7 32 26\n",
      " 27 21 30 32]\n",
      "\n",
      "Input sequence:\n",
      "[ 3  7 15  8  9  7 32 12  7 32 29  9  8 33  7  7 18 32 21 30 31 32 15 28\n",
      "  9 26 27  7  9 11 32 27  7 21  9 32 24  7 32 22 29  7 21  6 37 32 22 29\n",
      "  7 21  6 11 32 22 29  7 21  6 37 32 31  8 28 32 21  9  7 32 21  1  1 32\n",
      "  9  7 22  8  1 23  7 18 32  9 21 26 27  7  9 32 26  8 32 18 10  7 32 26\n",
      " 27 21 30 32]\n",
      "\n",
      "Target sequence:\n",
      "[ 7 15  8  9  7 32 12  7 32 29  9  8 33  7  7 18 32 21 30 31 32 15 28  9\n",
      " 26 27  7  9 11 32 27  7 21  9 32 24  7 32 22 29  7 21  6 37 32 22 29  7\n",
      " 21  6 11 32 22 29  7 21  6 37 32 31  8 28 32 21  9  7 32 21  1  1 32  9\n",
      "  7 22  8  1 23  7 18 32  9 21 26 27  7  9 32 26  8 32 18 10  7 32 26 27\n",
      " 21 30 32 26]\n"
     ]
    }
   ],
   "source": [
    "# Encode the train dataset\n",
    "train_data = encode_text(sentences, vocab, one_hot = False)\n",
    "\n",
    "# Create the input sequence, from 0 to len-1\n",
    "input_seq=train_data[:-1]\n",
    "# Create the target sequence, from 1 to len. It is right-shifted one place\n",
    "target_seq=train_data[1:]\n",
    "print('\\nOriginal text:')\n",
    "print(sentences[:100])\n",
    "print('\\nEncoded text:')\n",
    "print(train_data[:100])\n",
    "print('\\nInput sequence:')\n",
    "print(input_seq[:100])\n",
    "print('\\nTarget sequence:')\n",
    "print(target_seq[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save our encoded dataset to a file, so we can restore it whenever it is necessary. It is important to note the format of the data that we are saving as we will need to know it when we write the training code. In our case, we will save the dataset as a pickle object, it is the array containing the whole dataset encoded as an integer value for every character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the encoded text to a file\n",
    "encoded_data = os.path.join(DATA_PATH, 'input_data.pkl')\n",
    "with open(encoded_data, 'wb') as fp:\n",
    "    pickle.dump(train_data, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check our one-hot-encode function that we will use later during the training phase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded characters:  [26  8]\n",
      "One-hot-encoded characters:  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print('Encoded characters: ',train_data[100:102])\n",
    "print('One-hot-encoded characters: ',one_hot_encode(train_data[100:102], 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a batch data generator\n",
    "\n",
    "When training on the dataset, we need to extract a batch size examples from the inputs and targets, forward and backward the RNN and then repite the iteration with another batch size examples. \n",
    "A batch generator will help us to extract a batch size examples from our datasets.\n",
    "\n",
    "First we will load a small portion of the training data set to use as a sample. It would be very time consuming to try and train the model completely in the notebook as we do not have access to a gpu and the compute instance that we are using is not particularly powerful. However, we can work on a small bit of the data to get a feel for how our training script is behaving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator_sequence(features_seq, label_seq, batch_size, seq_len):\n",
    "    \"\"\"Generator function that yields batches of data (input and target)\n",
    "\n",
    "    Args:\n",
    "        features_seq: sequence of chracters, feature of our model.\n",
    "        label_seq: sequence of chracters, the target label of our model\n",
    "        batch_size (int): number of examples (in this case, sentences) per batch.\n",
    "        seq_len (int): maximum length of the output tensor.\n",
    "\n",
    "    Yields:\n",
    "        x_epoch: sequence of features for the epoch\n",
    "        y_epoch: sequence of labels for the epoch\n",
    "    \"\"\"\n",
    "    # calculate the number of batches we can supply\n",
    "    num_batches = len(features_seq) // (batch_size * seq_len)\n",
    "    if num_batches == 0:\n",
    "        raise ValueError(\"No batches created. Use smaller batch size or sequence length.\")\n",
    "    # calculate effective length of text to use\n",
    "    rounded_len = num_batches * batch_size * seq_len\n",
    "    # Reshape the features matrix in batch size x num_batches * seq_len\n",
    "    x = np.reshape(features_seq[: rounded_len], [batch_size, num_batches * seq_len])\n",
    "    \n",
    "    # Reshape the target matrix in batch size x num_batches * seq_len\n",
    "    y = np.reshape(label_seq[: rounded_len], [batch_size, num_batches * seq_len])\n",
    "    \n",
    "    epoch = 0\n",
    "    while True:\n",
    "        # roll so that no need to reset rnn states over epochs\n",
    "        x_epoch = np.split(np.roll(x, -epoch, axis=0), num_batches, axis=1)\n",
    "        y_epoch = np.split(np.roll(y, -epoch, axis=0), num_batches, axis=1)\n",
    "        for batch in range(num_batches):\n",
    "            yield x_epoch[batch], y_epoch[batch]\n",
    "        epoch += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the RNN model\n",
    "\n",
    "The model is very simple_\n",
    "- An LSTM layer to encode the input (there is no need for an embedding layer because the data is one-hot-encoded)\n",
    "- A dropout layer to reduce overfitting\n",
    "- The decoder, a fully connected layer mapping to a vocabulary size outputs\n",
    "\n",
    "The output provides the probability of every item in the vocabulary to be the next char. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_dim, n_layers, drop_rate=0.2):\n",
    "        \n",
    "        super(RNNModel, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_size = embedding_size\n",
    "        self.n_layers = n_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.drop_rate = drop_rate\n",
    "        self.char2int = None\n",
    "        self.int2char = None\n",
    "\n",
    "\n",
    "        #Defining the layers\n",
    "        # Define the encoder as an Embedding layer\n",
    "        #self.encoder = nn.Embedding(vocab_size, embedding_size)\n",
    "            \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "        # RNN Layer\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_dim, n_layers, dropout=drop_rate, batch_first = True)\n",
    "        # Fully connected layer\n",
    "        self.decoder = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x, state):\n",
    "        \n",
    "        # input shape: [batch_size, seq_len, embedding_size]\n",
    "        # Apply the embedding layer and dropout\n",
    "        #embed_seq = self.dropout(self.encoder(x))\n",
    "            \n",
    "        #print('Input RNN shape: ', embed_seq.shape)\n",
    "        # shape: [batch_size, seq_len, embedding_size]\n",
    "        rnn_out, state = self.rnn(x, state)\n",
    "        #print('Out RNN shape: ', rnn_out.shape)\n",
    "        # rnn_out shape: [batch_size, seq_len, rnn_size]\n",
    "        # hidden shape: [2, num_layers, batch_size, rnn_size]\n",
    "        rnn_out = self.dropout(rnn_out)\n",
    "\n",
    "        # shape: [seq_len, batch_size, rnn_size]\n",
    "        # Stack up LSTM outputs using view\n",
    "        # you may need to use contiguous to reshape the output\n",
    "        rnn_out = rnn_out.contiguous().view(-1, self.hidden_dim)\n",
    "\n",
    "        logits = self.decoder(rnn_out)\n",
    "        # output shape: [seq_len * batch_size, vocab_size]\n",
    "        #print('Output model shape: ', logits.shape)\n",
    "        return logits, state\n",
    "    \n",
    "    def init_state(self, device, batch_size=1):\n",
    "        \"\"\"\n",
    "        initialises rnn states.\n",
    "        \"\"\"\n",
    "        #return (Variable(torch.zeros(self.n_layers, batch_size, self.hidden_dim)),\n",
    "        #        Variable(torch.zeros(self.n_layers, batch_size, self.hidden_dim)))\n",
    "        return (torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device),\n",
    "                torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device))\n",
    "\n",
    "    def predict(self, input):\n",
    "        # input shape: [seq_len, batch_size]\n",
    "        logits, hidden = self.forward(input)\n",
    "        # logits shape: [seq_len * batch_size, vocab_size]\n",
    "        # hidden shape: [2, num_layers, batch_size, rnn_size]\n",
    "        probs = F.softmax(logits)\n",
    "        # shape: [seq_len * batch_size, vocab_size]\n",
    "        probs = probs.view(input.size(0), input.size(1), probs.size(1))\n",
    "        # output shape: [seq_len, batch_size, vocab_size]\n",
    "        return probs, hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining a RNN model, we can code the main training function. It is very simple and the steps involved are the usual ones in any other training of a neural network: in every epoch get the next batch data, move the tensors to the device, call the model (Forward pass), calculate the loss function, get the gradients and update the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_main(model, optimizer, loss_fn, batch_data, num_batches, val_batches, batch_size, seq_len, n_epochs, clip_norm, device):\n",
    "    # Training Run\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # Store the loss in every batch iteration\n",
    "        #epoch_losses = torch.Tensor(num_batches)\n",
    "        epoch_losses = []\n",
    "        # Init the hidden state\n",
    "        hidden = model.init_state(device, batch_size)\n",
    "        # Train all the batches in every epoch\n",
    "        for i in tqdm(range(num_batches-val_batches), desc=\"Epoch {}/{}\".format(epoch, n_epochs+1)):\n",
    "            #print('Batch :', i)\n",
    "            # Get the next batch data for input and target\n",
    "            input_batch, target_batch = next(batch_data)\n",
    "            # Onr hot encode the input data\n",
    "            input_batch = one_hot_encode(input_batch, model.vocab_size)\n",
    "            # Tranform to tensor\n",
    "            input_data = torch.from_numpy(input_batch)\n",
    "            target_data = torch.from_numpy(target_batch)\n",
    "            # Create a new variable for the hidden state, necessary to calculate the gradients\n",
    "            hidden = tuple(([Variable(var.data) for var in hidden]))\n",
    "            # Move the input data to the device\n",
    "            input_data = input_data.to(device)\n",
    "            #print('Input shape: ', input_data.shape)\n",
    "            #print('Hidden shape: ', hidden[0].shape, hidden[1].shape)\n",
    "            # Set the model to train and prepare the gradients\n",
    "            model.train()\n",
    "            optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "            # Pass Fordward the RNN\n",
    "            output, hidden = model(input_data, hidden)\n",
    "            #print('Output shape: ', output.shape)\n",
    "            output = output.to(device)\n",
    "            #print('Output shape: ', output.shape)\n",
    "            #print('Target shape; ', target_data.shape)\n",
    "            # Move the target data to the device\n",
    "            target_data = target_data.to(device)\n",
    "            #print('Target shape; ', target_data.shape)\n",
    "            target_data = torch.reshape(target_data, (batch_size*seq_len,))\n",
    "            #print('Target shape; ', target_data.shape)\n",
    "            loss = loss_fn(output, target_data.view(batch_size*seq_len))\n",
    "            #print(loss)\n",
    "            # Save the loss\n",
    "            #epoch_losses[i] = loss.item() #data[0]\n",
    "            epoch_losses.append(loss.item()) #data[0]\n",
    "        \n",
    "            loss.backward() # Does backpropagation and calculates gradients\n",
    "            # clip gradient norm\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
    "            \n",
    "            optimizer.step() # Updates the weights accordingly\n",
    "    \n",
    "        # Now, when epoch is finished, evaluate the model on validation data\n",
    "        model.eval()\n",
    "        val_hidden = model.init_state(device, batch_size)\n",
    "        val_losses = []\n",
    "        for i in tqdm(range(val_batches), desc=\"Val Epoch {}/{}\".format(epoch, n_epochs+1)):\n",
    "            # Get the next batch data for input and target\n",
    "            input_batch, target_batch = next(batch_data)\n",
    "            # Onr hot encode the input data\n",
    "            input_batch = one_hot_encode(input_batch, model.vocab_size)\n",
    "            # Tranform to tensor\n",
    "            input_data = torch.from_numpy(input_batch)\n",
    "            target_data = torch.from_numpy(target_batch)\n",
    "            # Create a new variable for the hidden state, necessary to calculate the gradients\n",
    "            hidden = tuple(([Variable(var.data) for var in val_hidden]))\n",
    "            # Move the input data to the device\n",
    "            input_data = input_data.to(device)\n",
    "            # Pass Fordward the RNN\n",
    "            output, hidden = model(input_data, hidden)\n",
    "            #print('Output shape: ', output.shape)\n",
    "            output = output.to(device)\n",
    "            #print('Output shape: ', output.shape)\n",
    "            #print('Target shape; ', target_data.shape)\n",
    "            # Move the target data to the device\n",
    "            target_data = target_data.to(device)\n",
    "            #print('Target shape; ', target_data.shape)\n",
    "            target_data = torch.reshape(target_data, (batch_size*seq_len,))\n",
    "            #print('Target shape; ', target_data.shape)\n",
    "            loss = loss_fn(output, target_data.view(batch_size*seq_len))\n",
    "            #print(loss)\n",
    "            # Save the loss\n",
    "            val_losses.append(loss.item()) #data[0]\n",
    "\n",
    "        model.train()                  \n",
    "        #if epoch%2 == 0:\n",
    "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "        print(\"Train Loss: {:.4f}\".format(np.mean(epoch_losses)), end=' ')\n",
    "        print(\"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
    "        \n",
    "    return epoch_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start building the model, let's use a build in feature in PyTorch to check the device we're running on (CPU or GPU). This implementation will not require GPU as the training is really simple. However, as you progress on to large datasets and models with millions of trainable parameters, using the GPU will be very important to speed up your training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_device():\n",
    "    # torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "    is_cuda = torch.cuda.is_available()\n",
    "\n",
    "    # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "    if is_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"GPU is available\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"GPU not available, CPU used\")\n",
    "    \n",
    "    return device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the model above, we'll have to instantiate the model with the relevant parameters and define our hyperparamters as well. The hyperparameters we're defining below are:\n",
    "\n",
    "- n_epochs: Number of Epochs --> This refers to the number of times our model will go through the entire training dataset\n",
    "- lr: Learning Rate --> This affects the rate at which our model updates the weights in the cells each time backpropogation is done\n",
    "    - A smaller learning rate means that the model changes the values of the weight with a smaller magnitude\n",
    "    - A larger learning rate means that the weights are updated to a larger extent for each time step\n",
    "- batch_size: Number of examples to train on every train step\n",
    "- maxlen: Length of the input sequence of char\n",
    "- embedding_size: the vocab size because the input feature is one-hot-encoded\n",
    "- hidden_dim: the number of hidden units in our LSTM module\n",
    "- n_layers: number of layers of our LSTM module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n",
      "Device:  cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5761eba3b0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define hyperparameters for training\n",
    "n_epochs = 5\n",
    "lr=0.01\n",
    "batch_size=64\n",
    "maxlen=64\n",
    "clip_norm=5\n",
    "val_fraction = 0.1\n",
    "\n",
    "# Define hypeparameters of the model\n",
    "hidden_dim = 64 #64\n",
    "n_layers = 1\n",
    "embedding_size=len(vocab.char2int)\n",
    "dict_size = len(vocab.char2int)\n",
    "drop_rate = 0.2\n",
    "\n",
    "# Set the device for training\n",
    "device = set_device()\n",
    "print('Device: ', device)\n",
    "# Set a seed to reproduce experiments\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to other neural networks, we have to define the optimizer and loss function as well. We’ll be using CrossEntropyLoss as the final output is basically a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNModel(\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (rnn): LSTM(38, 64, batch_first=True, dropout=0.2)\n",
      "  (decoder): Linear(in_features=64, out_features=38, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model with hyperparameters\n",
    "model = RNNModel(dict_size,embedding_size, hidden_dim, n_layers)\n",
    "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
    "model = model.to(device)\n",
    "print(model)\n",
    "# Define Loss, Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "# Limit the size of our input sequence to limit the training time, we are just testing the model\n",
    "input_seq = input_seq[:100000]\n",
    "target_seq = target_seq[:100000]\n",
    "print(len(input_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/6: 100%|██████████| 22/22 [00:02<00:00,  8.41it/s]\n",
      "Val Epoch 1/6: 100%|██████████| 2/2 [00:00<00:00, 61.46it/s]\n",
      "Epoch 2/6:  14%|█▎        | 3/22 [00:00<00:00, 24.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5............. Train Loss: 3.0507 Val Loss: 2.8581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/6: 100%|██████████| 22/22 [00:00<00:00, 24.67it/s]\n",
      "Val Epoch 2/6: 100%|██████████| 2/2 [00:00<00:00, 61.66it/s]\n",
      "Epoch 3/6:  14%|█▎        | 3/22 [00:00<00:00, 24.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/5............. Train Loss: 2.6476 Val Loss: 2.4374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/6: 100%|██████████| 22/22 [00:00<00:00, 24.60it/s]\n",
      "Val Epoch 3/6: 100%|██████████| 2/2 [00:00<00:00, 62.49it/s]\n",
      "Epoch 4/6:  14%|█▎        | 3/22 [00:00<00:00, 24.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/5............. Train Loss: 2.3630 Val Loss: 2.2691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/6: 100%|██████████| 22/22 [00:00<00:00, 24.14it/s]\n",
      "Val Epoch 4/6: 100%|██████████| 2/2 [00:00<00:00, 56.03it/s]\n",
      "Epoch 5/6:  14%|█▎        | 3/22 [00:00<00:00, 23.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/5............. Train Loss: 2.2456 Val Loss: 2.1838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/6: 100%|██████████| 22/22 [00:00<00:00, 24.33it/s]\n",
      "Val Epoch 5/6: 100%|██████████| 2/2 [00:00<00:00, 61.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/5............. Train Loss: 2.1780 Val Loss: 2.1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of batches to train\n",
    "num_batches = len(input_seq) // (batch_size*maxlen)\n",
    "val_batches = int(num_batches*val_fraction)\n",
    "# Create the batch data generator\n",
    "batch_data = batch_generator_sequence(input_seq, target_seq, batch_size, maxlen)\n",
    "losses = train_main(model, optimizer, criterion, batch_data, num_batches, val_batches, batch_size, \n",
    "                    maxlen, n_epochs, clip_norm, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained, we save it to disk then we can reload later and use ir for prediction. We also save the model parameters, they will be used to recreate the model if it is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the parameters used to construct the model\n",
    "model_info_path = os.path.join(model_dir, 'model_info.pth')\n",
    "with open(model_info_path, 'wb') as f:\n",
    "    model_info = {\n",
    "        'n_layers': n_layers,\n",
    "        'embedding_dim': embedding_size,\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'vocab_size': dict_size,\n",
    "        'drop_rate': drop_rate\n",
    "    }\n",
    "    torch.save(model_info, f)\n",
    "\n",
    "# Save the model parameters\n",
    "model_path = os.path.join(model_dir, 'model.pth')\n",
    "with open(model_path, 'wb') as f:\n",
    "    torch.save(model.state_dict(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict an input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_probs(probs, top_n=10):\n",
    "    \"\"\"\n",
    "    truncated weighted random choice.\n",
    "    \"\"\"\n",
    "    _, indices = torch.sort(probs)\n",
    "    # set probabilities after top_n to 0\n",
    "    probs[indices.data[:-top_n]] = 0\n",
    "    #print(probs.shape)\n",
    "    sampled_index = torch.multinomial(probs, 1)\n",
    "    return sampled_index\n",
    "\n",
    "def predict_probs(model, hidden, character, vocab):\n",
    "    # One-hot encoding our input to fit into the model\n",
    "    character = np.array([[vocab.char2int[c] for c in character]])\n",
    "    #character = one_hot_encode(character, len(vocab.char2int), character.shape[1], 1)\n",
    "    character = one_hot_encode(character, model.vocab_size)\n",
    "    character = torch.from_numpy(character)\n",
    "    character = character.to(device)\n",
    "    \n",
    "    out, hidden = model(character, hidden)\n",
    "\n",
    "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
    "\n",
    "    return prob, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s test our model now and see what kind of output we will get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_text(model, out_len, vocab, top_n=1, start='hey'):\n",
    "    model.eval() # eval mode\n",
    "    start = start.lower()\n",
    "    # First off, run through the starting characters\n",
    "    chars = [ch for ch in start]\n",
    "    size = out_len - len(chars)\n",
    "    # Generate the initial hidden state\n",
    "    device = set_device()\n",
    "    state = model.init_state(device, 1)\n",
    "    \n",
    "    # Warm up the initial state, predicting on the initial string\n",
    "    for ch in chars:\n",
    "        #char, state = predict(model, ch, state, top_n=top_k)\n",
    "        probs, state = predict_probs(model, state, ch, vocab)\n",
    "        next_index = sample_from_probs(probs, top_n)\n",
    "\n",
    "    # Now pass in the previous characters and get a new one\n",
    "    for ii in range(size):\n",
    "        #char, h = predict_char(model, chars, vocab)\n",
    "        probs, state = predict_probs(model, state, chars, vocab)\n",
    "        next_index = sample_from_probs(probs, top_n)\n",
    "        # append to sequence\n",
    "        chars.append(vocab.int2char[next_index.data[0]])\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n",
      "we want the he ares. ar to hat\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "text_predicted = generate_from_text(model, 30, vocab, 3, 'we want ')\n",
    "print(text_predicted)\n",
    "print(len(text_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function  will feed our model one character at a time instead of providing it with the entire string of text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_char(model, out_len, vocab, top_n=1, start='hey'):\n",
    "    model.eval() # eval mode\n",
    "    start = start.lower()\n",
    "    # First off, run through the starting characters\n",
    "    chars = [ch for ch in start]\n",
    "    size = out_len - len(chars)\n",
    "    # Generate the initial hidden state\n",
    "    device = set_device()\n",
    "    state = model.init_state(device, 1)\n",
    "    # Warm up the initial state, predicting on the initial string\n",
    "    for ch in chars:\n",
    "        #char, state = predict(model, ch, state, top_n=top_k)\n",
    "        probs, state = predict_probs(model, state, ch, vocab)\n",
    "        next_index = sample_from_probs(probs, top_n)\n",
    "        \n",
    "    # Include the last char predicted to the predicted output\n",
    "    chars.append(vocab.int2char[next_index.data[0]])   \n",
    "    \n",
    "    # Now pass in the previous characters and get a new one\n",
    "    for ii in range(size-1):\n",
    "        #char, h = predict_char(model, chars, vocab)\n",
    "        probs, state = predict_probs(model, state, chars[-1], vocab)\n",
    "        next_index = sample_from_probs(probs, top_n)\n",
    "        # append to sequence\n",
    "        chars.append(vocab.int2char[next_index.data[0]])\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n",
      "we want what, and and whin she\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "text_predicted = generate_from_char(model, 30, vocab, 3, 'we want ')\n",
    "print(text_predicted)\n",
    "print(len(text_predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have developed a function to predict the next char given a initial string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_char(model, character, vocab):\n",
    "    # One-hot encoding our input to fit into the model\n",
    "    character = np.array([[vocab.char2int[c] for c in character]])\n",
    "    #character = one_hot_encode(character, len(vocab.char2int), character.shape[1], 1)\n",
    "    character = one_hot_encode(character, model.vocab_size)\n",
    "    character = torch.from_numpy(character)\n",
    "    # Generate set the device\n",
    "    device = set_device()\n",
    "    character = character.to(device)\n",
    "    \n",
    "    model.eval() # eval mode\n",
    "    # Generate the initial hidden state\n",
    "    state = model.init_state(device, 1)\n",
    "\n",
    "    out, hidden = model(character, state)\n",
    "\n",
    "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
    "    # Taking the class with the highest probability score from the output\n",
    "    m = torch.max(prob, dim=0)\n",
    "    char_ind = torch.max(prob, dim=0)[1].item()\n",
    "\n",
    "    return vocab.int2char[char_ind], hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n",
      "Initial string:  t\n"
     ]
    }
   ],
   "source": [
    "t,_ = predict_char(model, 'we want ', vocab)\n",
    "print('Initial string: ', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we are ready to train our model in Amazon SageMaker using the whole data set and improving its performance training on many epochs for a longer time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
